{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.parameters import Parameters\n",
    "from src.features import Featureset\n",
    "from src.data_loaders import GroupDataset, GroupDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# raw_data = pd.read_csv('datasets/ehr/data_ver2.csv')\n",
    "# raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    'pat_id':Parameters(mode='uid', vector=None),\n",
    "    'label':Parameters(mode='categorical', vector=None),\n",
    "    'race':Parameters(mode='categorical', vector='embedding'),\n",
    "    'ethnic':Parameters(mode='categorical', vector='embedding'),\n",
    "    'sex':Parameters(mode='categorical', vector='onehot'),\n",
    "    'marital_status':Parameters(mode='categorical', vector='onehot'),\n",
    "    'age':Parameters(mode='numerical', vector='linear'),\n",
    "    'time':Parameters(mode='datetime', vector=None)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Save Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ehr_utils import EHRFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataframe\n",
      "converting datetimes\n",
      "updating config\n",
      "creating features\n"
     ]
    }
   ],
   "source": [
    "_features_ehr = Featureset('datasets/ehr', data_config)\n",
    "_features_ehr.load_ehr('data_ver2.csv')\n",
    "_features_ehr.create_new_dataset(train_split=0.9, tag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': torch.Size([2, 1]),\n",
       " 'race': torch.Size([2, 1]),\n",
       " 'ethnic': torch.Size([2, 1]),\n",
       " 'sex': torch.Size([2, 1, 4]),\n",
       " 'marital_status': torch.Size([2, 1, 10]),\n",
       " 'age': torch.Size([2, 1, 1]),\n",
       " 'srel': torch.Size([2, 163, 1]),\n",
       " 'wday': torch.Size([2, 163]),\n",
       " 'seq_mask': torch.Size([2, 163])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "features = Featureset('datasets/ehr')\n",
    "features.load_dataset(tag=1)\n",
    "                          \n",
    "dataset = GroupDataset(features.test_features)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn = GroupDataCollator(features.config))\n",
    "data = next(iter(data_loader))\n",
    "\n",
    "{k:v.shape for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': Parameters(mode='categorical', vector=None, sub_mode=None, token_map={0: 0, 1: 1}, size=2, min=None, max=None, range=None, max_len=None, d_layer_1=None, d_layer_2=None, d_layer_3=None, d_layer_4=None),\n",
       " 'race': Parameters(mode='categorical', vector='embedding', sub_mode=None, token_map={0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}, size=8, min=None, max=None, range=None, max_len=None, d_layer_1=None, d_layer_2=None, d_layer_3=None, d_layer_4=None),\n",
       " 'ethnic': Parameters(mode='categorical', vector='embedding', sub_mode=None, token_map={0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8}, size=9, min=None, max=None, range=None, max_len=None, d_layer_1=None, d_layer_2=None, d_layer_3=None, d_layer_4=None),\n",
       " 'sex': Parameters(mode='categorical', vector='onehot', sub_mode=None, token_map={1: 0, 2: 1, 0: 2, 3: 3}, size=4, min=None, max=None, range=None, max_len=None, d_layer_1=None, d_layer_2=None, d_layer_3=None, d_layer_4=None),\n",
       " 'marital_status': Parameters(mode='categorical', vector='onehot', sub_mode=None, token_map={1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 0: 7, 8: 8, 9: 9}, size=10, min=None, max=None, range=None, max_len=None, d_layer_1=None, d_layer_2=None, d_layer_3=None, d_layer_4=None),\n",
       " 'age': Parameters(mode='numerical', vector='linear', sub_mode=None, token_map=None, size=1, min=-1.0, max=1.0, range=None, max_len=None, d_layer_1=None, d_layer_2=None, d_layer_3=None, d_layer_4=None),\n",
       " 'srel': Parameters(mode='numerical', vector='linear', sub_mode=None, token_map=None, size=1, min=0.7246685557221395, max=1.0, range=None, max_len=163, d_layer_1=None, d_layer_2=None, d_layer_3=None, d_layer_4=None),\n",
       " 'wday': Parameters(mode='categorical', vector='embedding', sub_mode=None, token_map={0: 0, 3: 1, 1: 2, 4: 3, 2: 4, 5: 5, 6: 6, 7: 7}, size=8, min=None, max=None, range=None, max_len=163, d_layer_1=None, d_layer_2=None, d_layer_3=None, d_layer_4=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:features.config[k] for k,v in data.items() if k in features.config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Explore Group Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules import MergeLayer, FlattenLayer, GroupLayer, GroupModel\n",
    "from src.modules import NonLinear, EmbeddingNonLinear\n",
    "from src.modules import GRU, EmbeddingGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GroupModel({\n",
    "    'embed':GroupLayer({\n",
    "        'race':EmbeddingNonLinear(8, 4),\n",
    "        'ethnic':EmbeddingNonLinear(9, 4),\n",
    "        'sex':NonLinear(4, 8, 4),\n",
    "        'marital_status':NonLinear(10, 8, 4),\n",
    "        'age':NonLinear(1, 2, 4),\n",
    "        'srel':GRU(1, 2, 4),\n",
    "        'wday':EmbeddingGRU(8, 4, 2, 4)\n",
    "    }), \n",
    "    \n",
    "    'merge':MergeLayer({\n",
    "        'bkgd':('race', 'ethnic'),\n",
    "        'love':('sex', 'marital_status'),\n",
    "        'age':('age', ),\n",
    "        'times':('srel', 'wday')\n",
    "    }),\n",
    "    \n",
    "    'groups_1':GroupLayer({\n",
    "        'bkgd':NonLinear(8, 16, 8),\n",
    "        'love':NonLinear(8, 16, 8),\n",
    "        'age':NonLinear(4, 8, 4),\n",
    "        'times':NonLinear(8, 6, 4)\n",
    "    }),\n",
    "    \n",
    "    'flat':FlattenLayer('times'),\n",
    "    \n",
    "    'proj':MergeLayer({\n",
    "        'pred':('bkgd', 'love', 'age', 'times'),\n",
    "    }), \n",
    "    \n",
    "    'pred':GroupLayer({\n",
    "        'pred':NonLinear(24, 64, 2)\n",
    "    })\n",
    "})\n",
    "\n",
    "model.eval()\n",
    "pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1116, 0.0681],\n",
       "        [0.1044, 0.0642]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run data through first layer 'vars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'race': tensor([[[-0.1746, -0.2779,  0.1017,  0.2228]],\n",
       " \n",
       "         [[-0.1746, -0.2779,  0.1017,  0.2228]]], grad_fn=<AddBackward0>),\n",
       " 'ethnic': tensor([[[ 0.0458,  0.2116, -0.2210, -0.2679]],\n",
       " \n",
       "         [[ 0.0458,  0.2116, -0.2210, -0.2679]]], grad_fn=<AddBackward0>),\n",
       " 'sex': tensor([[[-0.2944,  0.4751,  0.4800, -0.1711]],\n",
       " \n",
       "         [[-0.1906,  0.2703,  0.4496, -0.0269]]], grad_fn=<AddBackward0>),\n",
       " 'marital_status': tensor([[[ 0.0701, -0.3316, -0.1484,  0.0023]],\n",
       " \n",
       "         [[ 0.1060, -0.4011, -0.0540, -0.0918]]], grad_fn=<AddBackward0>),\n",
       " 'age': tensor([[[-0.4764, -0.6471, -0.5324, -0.5595]],\n",
       " \n",
       "         [[-0.4530, -0.6569, -0.5448, -0.5585]]], grad_fn=<AddBackward0>),\n",
       " 'srel': tensor([[[ 0.7127, -0.4101,  0.0724, -0.2590]],\n",
       " \n",
       "         [[-0.1450,  0.2946, -1.0849, -0.3482]]], grad_fn=<MeanBackward1>),\n",
       " 'wday': tensor([[[ 0.4433, -0.3746,  0.1728, -0.5243]],\n",
       " \n",
       "         [[ 0.2181, -0.2473,  0.2206, -0.4935]]], grad_fn=<MeanBackward1>)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers['embed'](data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run data through first two layers 'vars', 'groups_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bkgd': tensor([[[-0.1746, -0.2779,  0.1017,  0.2228,  0.0458,  0.2116, -0.2210,\n",
       "           -0.2679]],\n",
       " \n",
       "         [[-0.1746, -0.2779,  0.1017,  0.2228,  0.0458,  0.2116, -0.2210,\n",
       "           -0.2679]]], grad_fn=<CatBackward>),\n",
       " 'love': tensor([[[-0.2944,  0.4751,  0.4800, -0.1711,  0.0701, -0.3316, -0.1484,\n",
       "            0.0023]],\n",
       " \n",
       "         [[-0.1906,  0.2703,  0.4496, -0.0269,  0.1060, -0.4011, -0.0540,\n",
       "           -0.0918]]], grad_fn=<CatBackward>),\n",
       " 'age': tensor([[[-0.4764, -0.6471, -0.5324, -0.5595]],\n",
       " \n",
       "         [[-0.4530, -0.6569, -0.5448, -0.5585]]], grad_fn=<CatBackward>),\n",
       " 'times': tensor([[[ 0.2144, -0.0826, -0.5119, -0.7036,  0.5094, -0.3008, -0.0827,\n",
       "           -0.6575]],\n",
       " \n",
       "         [[ 0.0427,  0.2964, -0.8879, -0.2301,  0.0534, -0.0280, -0.1910,\n",
       "           -0.1676]]], grad_fn=<CatBackward>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers['merge'](model.layers['embed'](data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 'race' through 'race' block of first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1746, -0.2779,  0.1017,  0.2228]],\n",
       "\n",
       "        [[-0.1746, -0.2779,  0.1017,  0.2228]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers['embed'].blocks['race'](data['race'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 'srel' through 'srel' block of first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1734,  0.1998, -1.0488, -0.2826]],\n",
       "\n",
       "        [[ 0.2049, -0.0022, -0.4994,  0.2558]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers['embed'].blocks['srel'](data['srel'], data['seq_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Train Group Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Featureset('datasets/ehr')\n",
    "features.load_dataset(tag=1)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    GroupDataset(features.train_features), batch_size=100, shuffle=True, \n",
    "    collate_fn=GroupDataCollator(features.config)\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    GroupDataset(features.test_features), batch_size=100, shuffle=False, \n",
    "    collate_fn=GroupDataCollator(features.config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 2\n",
    "epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 11:10:25\n",
      "Epoch 000 || Train Loss: 0.681 || Test Loss: 0.673\n",
      "Epoch 002 || Train Loss: 0.646 || Test Loss: 0.655\n",
      "Epoch 004 || Train Loss: 0.641 || Test Loss: 0.646\n",
      "Epoch 006 || Train Loss: 0.635 || Test Loss: 0.642\n",
      "Epoch 008 || Train Loss: 0.632 || Test Loss: 0.634\n",
      "End Time: 11:11:13\n",
      "Completed in 47.968401 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.datetime.now()\n",
    "print('Start Time: %s'%time_start.strftime('%H:%M:%S'))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    epoch_start = datetime.datetime.now()\n",
    "    model.train();\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    train_nbatches = 0\n",
    "    test_nbatches = 0\n",
    "    \n",
    "    for inputs in trainloader:\n",
    "        preds = model(inputs)\n",
    "\n",
    "        loss = criterion(preds, inputs['label'].squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += float(loss)\n",
    "        train_nbatches += 1\n",
    "    \n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        for inputs in testloader:\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, inputs['label'].squeeze())\n",
    "            test_loss += float(loss)\n",
    "            test_nbatches += 1\n",
    "\n",
    "    train_loss/=train_nbatches\n",
    "    test_loss/=test_nbatches\n",
    "    \n",
    "    if epoch%print_every == 0:\n",
    "        print('Epoch {} || Train Loss: {:.3f} || Test Loss: {:.3f}'.format(\n",
    "            str(epoch).zfill(3), train_loss, test_loss)\n",
    "             )\n",
    "time_finish = datetime.datetime.now()\n",
    "print('End Time: %s'%time_finish.strftime('%H:%M:%S'))\n",
    "print('Completed in %s seconds'%(time_finish-time_start).total_seconds())\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blah' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-4cbd040533a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mblah\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'blah' is not defined"
     ]
    }
   ],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch (Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_shift(x, klen=-1):\n",
    "    \"\"\"perform relative shift to form the relative attention score.\"\"\"\n",
    "    x_size = x.shape\n",
    "\n",
    "    x = x.reshape(x_size[1], x_size[0], x_size[2], x_size[3])\n",
    "    x = x[1:, ...]\n",
    "    x = x.reshape(x_size[0], x_size[1] - 1, x_size[2], x_size[3])\n",
    "    x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))\n",
    "    return x\n",
    "\n",
    "def rel_shift_bnij(x, klen=-1):\n",
    "    x_size = x.shape\n",
    "    x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])\n",
    "    x = x[:, :, 1:, :]\n",
    "    x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)\n",
    "    x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_n_heads(d, d_head):\n",
    "    if d<d_head:\n",
    "        return 1\n",
    "    else:\n",
    "        return int(np.ceil(d/d_head))\n",
    "\n",
    "class GroupAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.scale = 1 / (config['d_head'] ** 0.5)\n",
    "        \n",
    "        \n",
    "        q,z,r = self.init_queries(config['q'], config['d_head'])\n",
    "        k,v = self.init_keys(config['k'], config['d_head'])\n",
    "        \n",
    "        self.q = nn.Parameter(q)\n",
    "        self.z = nn.Parameter(z)\n",
    "        self.r = nn.Parameter(r)\n",
    "        self.k = nn.Parameter(k)\n",
    "        self.v = nn.Parameter(v)\n",
    "#         self.r_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))\n",
    "        self.w_bias = nn.Parameter(torch.FloatTensor(1, config['d_head']))\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(12, eps=1e-5)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        \n",
    "    def init_queries(self, q_config, d_head):\n",
    "        q_dims = list(q_config.values())\n",
    "        n_heads = [get_n_heads(d, d_head) for d in q_dims]\n",
    "        \n",
    "        self.q_head_view = dict(\n",
    "            zip(q_config.keys(),\n",
    "                map(\n",
    "                    lambda x: slice(*x), \n",
    "                    zip(\n",
    "                        [0]+n_heads, \n",
    "                        [sum(n_heads[:i]) for i in range(1, len(n_heads)+1)]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        q = torch.FloatTensor(sum(q_dims), sum(n_heads), d_head).uniform_(-0.1, 0.1)\n",
    "        z = torch.FloatTensor(sum(q_dims), sum(n_heads), d_head).uniform_(-0.1, 0.1)\n",
    "        r = torch.FloatTensor(sum(q_dims), sum(n_heads), d_head).uniform_(-0.1, 0.1)\n",
    "#         print(q)\n",
    "        return q, z, r\n",
    "    \n",
    "    def init_keys(self, k_config, d_head):\n",
    "        k_dims = list(k_config.values())\n",
    "        n_heads = [get_n_heads(d, d_head) for d in k_dims]\n",
    "        \n",
    "        self.k_head_view = dict(\n",
    "            zip(k_config.keys(),\n",
    "                map(\n",
    "                    lambda x: slice(*x), \n",
    "                    zip(\n",
    "                        [0]+n_heads, \n",
    "                        [sum(n_heads[:i]) for i in range(1, len(n_heads)+1)]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        k = torch.FloatTensor(sum(k_dims), sum(n_heads), d_head).uniform_(-0.1, 0.1)\n",
    "        v = torch.FloatTensor(sum(k_dims), sum(n_heads), d_head).uniform_(-0.1, 0.1)\n",
    "        return k, v\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def rel_shift(x, klen=-1):\n",
    "        \"\"\"perform relative shift to form the relative attention score.\"\"\"\n",
    "        x_size = x.shape\n",
    "\n",
    "        x = x.reshape(x_size[1], x_size[0], x_size[2], x_size[3])\n",
    "        x = x[1:, ...]\n",
    "        x = x.reshape(x_size[0], x_size[1] - 1, x_size[2], x_size[3])\n",
    "        x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def rel_shift_bnij(x, klen=-1):\n",
    "        x_size = x.shape\n",
    "        x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])\n",
    "        x = x[:, :, 1:, :]\n",
    "        x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)\n",
    "        x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))\n",
    "        return x\n",
    "\n",
    "    def rel_attn_core(\n",
    "        self,\n",
    "        q_head,\n",
    "        k_head_h,\n",
    "        v_head_h,\n",
    "        k_head_r=None,\n",
    "        attn_mask=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        \"\"\"Core relative positional attention operations.\"\"\"\n",
    "\n",
    "        # content based attention score\n",
    "        ac = torch.einsum(\"ibnd,jbnd->bnij\", q_head + self.w_bias, k_head_h)\n",
    "\n",
    "        # position based attention score\n",
    "#         bd = torch.einsum(\"ibnd,jbnd->bnij\", q_head + self.r_bias, k_head_r)\n",
    "#         bd = self.rel_shift_bnij(bd, klen=ac.shape[3])\n",
    "        bd = 0.\n",
    "\n",
    "        # merge attention scores and perform masking\n",
    "        attn_score = (ac + bd) * self.scale\n",
    "        if attn_mask is not None:\n",
    "            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n",
    "            if attn_mask.dtype == torch.float16:\n",
    "                attn_score = attn_score - 65500 * torch.einsum(\"ijbn->bnij\", attn_mask)\n",
    "            else:\n",
    "                attn_score = attn_score - 1e30 * torch.einsum(\"ijbn->bnij\", attn_mask)\n",
    "\n",
    "        # attention probability\n",
    "        attn_prob = F.softmax(attn_score, dim=3)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_prob = attn_prob * torch.einsum(\"ijbn->bnij\", head_mask)\n",
    "\n",
    "        # attention output\n",
    "        attn_vec = torch.einsum(\"bnij,jbnd->ibnd\", attn_prob, v_head_h)\n",
    "\n",
    "        if output_attentions:\n",
    "            return attn_vec, torch.einsum(\"bnij->ijbn\", attn_prob)\n",
    "\n",
    "        return attn_vec\n",
    "\n",
    "    def post_attention(self, h, attn_vec, residual=True):\n",
    "        \"\"\"Post-attention processing.\"\"\"\n",
    "        # post-attention projection (back to `d_model`)\n",
    "        attn_out = torch.einsum(\"ibnd,hnd->ibh\", attn_vec, self.z)\n",
    "\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        if residual:\n",
    "            attn_out = attn_out + h\n",
    "        output = self.layer_norm(attn_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        h,\n",
    "        attn_mask_h=None,\n",
    "        r=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # Multi-head attention with relative positional encoding\n",
    "        # content heads\n",
    "        h = torch.cat([data_emb[k] for k in attn_config_1['q'].keys()], dim=-1)\n",
    "        g = torch.cat([data_emb[k] for k in attn_config_1['k'].keys()], dim=-1)\n",
    "        q_head_h = torch.einsum(\"ibh,hnd->ibnd\", h, self.q)\n",
    "        k_head_h = torch.einsum(\"ibh,hnd->ibnd\", g, self.k)\n",
    "        v_head_h = torch.einsum(\"ibh,hnd->ibnd\", g, self.v)\n",
    "\n",
    "        # positional heads\n",
    "        # type casting for fp16 support\n",
    "#         k_head_r = torch.einsum(\"ibh,hnd->ibnd\", r.type(self.r.dtype), self.r)\n",
    "\n",
    "        # core attention ops\n",
    "        attn_vec = self.rel_attn_core(\n",
    "            q_head_h,\n",
    "            k_head_h,\n",
    "            v_head_h,\n",
    "            k_head_r=None,\n",
    "            attn_mask=attn_mask_h,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        if output_attentions:\n",
    "            attn_vec, attn_prob = attn_vec\n",
    "\n",
    "        # post processing\n",
    "        output_h = self.post_attention(h, attn_vec)\n",
    "\n",
    "        outputs = (output_h,)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attn_prob,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, config = load_dataset(1)\n",
    "dataset = GroupDataset(train_features)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=5, \n",
    "                                         collate_fn=GroupDataCollator(config))\n",
    "data = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testemb = GroupLayer({\n",
    "        'race':nn.Embedding(8, 4, padding_idx=0),\n",
    "        'ethnic':nn.Embedding(9, 8, padding_idx=0),\n",
    "        'sex':nn.Linear(5, 4),\n",
    "        'marital_status':nn.Linear(11, 8),\n",
    "        'age':nn.Linear(1, 4),\n",
    "        'srel':nn.Linear(1, 4),\n",
    "        'wday':nn.Embedding(9, 8)\n",
    "    })\n",
    "\n",
    "data_emb = testemb(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v.shape for k,v in data_emb.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_config_1 = {\n",
    "    'q':{'srel':4, 'wday':8},\n",
    "    'k':{'srel':4, 'wday':8},\n",
    "    'd_head':6\n",
    "}\n",
    "\n",
    "testattn = GroupAttention(attn_config_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testattn(data_emb, output_attentions=True)[0].shape\n",
    "testattn(data_emb, output_attentions=True)[1].shape\n",
    "testattn.q_head_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.rel_attn = XLNetRelativeAttention(config)\n",
    "        self.ff = XLNetFeedForward(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        output_h,\n",
    "        output_g,\n",
    "        attn_mask_h,\n",
    "        attn_mask_g,\n",
    "        r,\n",
    "        seg_mat,\n",
    "        mems=None,\n",
    "        target_mapping=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        outputs = self.rel_attn(\n",
    "            output_h,\n",
    "            attn_mask_h,\n",
    "            r,\n",
    "            seg_mat,\n",
    "            mems=mems,\n",
    "            target_mapping=target_mapping,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        output_h = outputs[:1]\n",
    "\n",
    "        output_h = self.ff(output_h)\n",
    "\n",
    "        outputs = (output_h) + outputs[2:]  # Add again attentions if there are there\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetModel(XLNetPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.mem_len = config.mem_len\n",
    "        self.reuse_len = config.reuse_len\n",
    "        self.d_model = config.d_model\n",
    "        self.same_length = config.same_length\n",
    "        self.attn_type = config.attn_type\n",
    "        self.bi_data = config.bi_data\n",
    "        self.clamp_len = config.clamp_len\n",
    "        self.n_layer = config.n_layer\n",
    "\n",
    "        self.word_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.mask_emb = nn.Parameter(torch.FloatTensor(1, 1, config.d_model))\n",
    "        self.layer = nn.ModuleList([XLNetLayer(config) for _ in range(config.n_layer)])\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.word_embedding\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.word_embedding = new_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_mask(self, qlen, mlen):\n",
    "        \"\"\"\n",
    "        Creates causal attention mask. Float mask where 1.0 indicates masked, 0.0 indicates not-masked.\n",
    "\n",
    "        Args:\n",
    "            qlen: Sequence length\n",
    "            mlen: Mask length\n",
    "\n",
    "        ::\n",
    "\n",
    "                  same_length=False:      same_length=True:\n",
    "                  <mlen > <  qlen >       <mlen > <  qlen >\n",
    "               ^ [0 0 0 0 0 1 1 1 1]     [0 0 0 0 0 1 1 1 1]\n",
    "                 [0 0 0 0 0 0 1 1 1]     [1 0 0 0 0 0 1 1 1]\n",
    "            qlen [0 0 0 0 0 0 0 1 1]     [1 1 0 0 0 0 0 1 1]\n",
    "                 [0 0 0 0 0 0 0 0 1]     [1 1 1 0 0 0 0 0 1]\n",
    "               v [0 0 0 0 0 0 0 0 0]     [1 1 1 1 0 0 0 0 0]\n",
    "\n",
    "        \"\"\"\n",
    "        attn_mask = torch.ones([qlen, qlen])\n",
    "        mask_up = torch.triu(attn_mask, diagonal=1)\n",
    "        attn_mask_pad = torch.zeros([qlen, mlen])\n",
    "        ret = torch.cat([attn_mask_pad, mask_up], dim=1)\n",
    "        if self.same_length:\n",
    "            mask_lo = torch.tril(attn_mask, diagonal=-1)\n",
    "            ret = torch.cat([ret[:, :qlen] + mask_lo, ret[:, qlen:]], dim=1)\n",
    "\n",
    "        ret = ret.to(self.device)\n",
    "        return ret\n",
    "\n",
    "    def cache_mem(self, curr_out, prev_mem):\n",
    "        # cache hidden states into memory.\n",
    "        if self.reuse_len is not None and self.reuse_len > 0:\n",
    "            curr_out = curr_out[: self.reuse_len]\n",
    "\n",
    "        if self.mem_len is None or self.mem_len == 0:\n",
    "            # If :obj:`use_cache` is active but no `mem_len` is defined, the model behaves like GPT-2 at inference time\n",
    "            # and returns all of the past and current hidden states.\n",
    "            cutoff = 0\n",
    "        else:\n",
    "            # If :obj:`use_cache` is active and `mem_len` is defined, the model returns the last `mem_len` hidden\n",
    "            # states. This is the preferred setting for training and long-form generation.\n",
    "            cutoff = -self.mem_len\n",
    "        if prev_mem is None:\n",
    "            # if :obj:`use_cache` is active and `mem_len` is defined, the model\n",
    "            new_mem = curr_out[cutoff:]\n",
    "        else:\n",
    "            new_mem = torch.cat([prev_mem, curr_out], dim=0)[cutoff:]\n",
    "\n",
    "        return new_mem.detach()\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_embedding(pos_seq, inv_freq, bsz=None):\n",
    "        sinusoid_inp = torch.einsum(\"i,d->id\", pos_seq, inv_freq)\n",
    "        pos_emb = torch.cat([torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], dim=-1)\n",
    "        pos_emb = pos_emb[:, None, :]\n",
    "\n",
    "        if bsz is not None:\n",
    "            pos_emb = pos_emb.expand(-1, bsz, -1)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    def relative_positional_encoding(self, qlen, klen, bsz=None):\n",
    "        # create relative positional encoding.\n",
    "        freq_seq = torch.arange(0, self.d_model, 2.0, dtype=torch.float)\n",
    "        inv_freq = 1 / torch.pow(10000, (freq_seq / self.d_model))\n",
    "\n",
    "        if self.attn_type == \"bi\":\n",
    "            # beg, end = klen - 1, -qlen\n",
    "            beg, end = klen, -qlen\n",
    "        elif self.attn_type == \"uni\":\n",
    "            # beg, end = klen - 1, -1\n",
    "            beg, end = klen, -1\n",
    "        else:\n",
    "            raise ValueError(\"Unknown `attn_type` {}.\".format(self.attn_type))\n",
    "\n",
    "        if self.bi_data:\n",
    "            fwd_pos_seq = torch.arange(beg, end, -1.0, dtype=torch.float)\n",
    "            bwd_pos_seq = torch.arange(-beg, -end, 1.0, dtype=torch.float)\n",
    "\n",
    "            if self.clamp_len > 0:\n",
    "                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n",
    "                bwd_pos_seq = bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n",
    "\n",
    "            if bsz is not None:\n",
    "                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz // 2)\n",
    "                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz // 2)\n",
    "            else:\n",
    "                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n",
    "                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n",
    "\n",
    "            pos_emb = torch.cat([fwd_pos_emb, bwd_pos_emb], dim=1)\n",
    "        else:\n",
    "            fwd_pos_seq = torch.arange(beg, end, -1.0)\n",
    "            if self.clamp_len > 0:\n",
    "                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n",
    "            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)\n",
    "\n",
    "        pos_emb = pos_emb.to(self.device)\n",
    "        return pos_emb\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        mems=None,\n",
    "        perm_mask=None,\n",
    "        target_mapping=None,\n",
    "        token_type_ids=None,\n",
    "        input_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        use_cache = self.training or (use_cache if use_cache is not None else self.config.use_cache)\n",
    "\n",
    "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
    "        # but we want a unified interface in the library with the batch size on the first dimension\n",
    "        # so we move here the first dimension (batch) to the end\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_ids = input_ids.transpose(0, 1).contiguous()\n",
    "            qlen, bsz = input_ids.shape[0], input_ids.shape[1]\n",
    "        elif inputs_embeds is not None:\n",
    "            inputs_embeds = inputs_embeds.transpose(0, 1).contiguous()\n",
    "            qlen, bsz = inputs_embeds.shape[0], inputs_embeds.shape[1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        token_type_ids = token_type_ids.transpose(0, 1).contiguous() if token_type_ids is not None else None\n",
    "        input_mask = input_mask.transpose(0, 1).contiguous() if input_mask is not None else None\n",
    "        attention_mask = attention_mask.transpose(0, 1).contiguous() if attention_mask is not None else None\n",
    "        perm_mask = perm_mask.permute(1, 2, 0).contiguous() if perm_mask is not None else None\n",
    "        target_mapping = target_mapping.permute(1, 2, 0).contiguous() if target_mapping is not None else None\n",
    "\n",
    "        mlen = mems[0].shape[0] if mems is not None and mems[0] is not None else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        dtype_float = self.dtype\n",
    "        device = self.device\n",
    "\n",
    "        # Attention mask\n",
    "        # causal attention mask\n",
    "        if self.attn_type == \"uni\":\n",
    "            attn_mask = self.create_mask(qlen, mlen)\n",
    "            attn_mask = attn_mask[:, :, None, None]\n",
    "        elif self.attn_type == \"bi\":\n",
    "            attn_mask = None\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported attention type: {}\".format(self.attn_type))\n",
    "\n",
    "        # data mask: input mask & perm mask\n",
    "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \"\n",
    "        \"or attention_mask (uses 0 for padding, added for compatibility with BERT). Please choose one.\"\n",
    "        if input_mask is None and attention_mask is not None:\n",
    "            input_mask = 1.0 - attention_mask\n",
    "        if input_mask is not None and perm_mask is not None:\n",
    "            data_mask = input_mask[None] + perm_mask\n",
    "        elif input_mask is not None and perm_mask is None:\n",
    "            data_mask = input_mask[None]\n",
    "        elif input_mask is None and perm_mask is not None:\n",
    "            data_mask = perm_mask\n",
    "        else:\n",
    "            data_mask = None\n",
    "\n",
    "        if data_mask is not None:\n",
    "            # all mems can be attended to\n",
    "            if mlen > 0:\n",
    "                mems_mask = torch.zeros([data_mask.shape[0], mlen, bsz]).to(data_mask)\n",
    "                data_mask = torch.cat([mems_mask, data_mask], dim=1)\n",
    "            if attn_mask is None:\n",
    "                attn_mask = data_mask[:, :, :, None]\n",
    "            else:\n",
    "                attn_mask += data_mask[:, :, :, None]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = (attn_mask > 0).to(dtype_float)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            non_tgt_mask = -torch.eye(qlen).to(attn_mask)\n",
    "            if mlen > 0:\n",
    "                non_tgt_mask = torch.cat([torch.zeros([qlen, mlen]).to(attn_mask), non_tgt_mask], dim=-1)\n",
    "            non_tgt_mask = ((attn_mask + non_tgt_mask[:, :, None, None]) > 0).to(attn_mask)\n",
    "        else:\n",
    "            non_tgt_mask = None\n",
    "\n",
    "        # Word embeddings and prepare h & g hidden states\n",
    "        if inputs_embeds is not None:\n",
    "            word_emb_k = inputs_embeds\n",
    "        else:\n",
    "            word_emb_k = self.word_embedding(input_ids)\n",
    "        output_h = self.dropout(word_emb_k)\n",
    "        if target_mapping is not None:\n",
    "            word_emb_q = self.mask_emb.expand(target_mapping.shape[0], bsz, -1)\n",
    "            # else:  # We removed the inp_q input which was same as target mapping\n",
    "            #     inp_q_ext = inp_q[:, :, None]\n",
    "            #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
    "            output_g = self.dropout(word_emb_q)\n",
    "        else:\n",
    "            output_g = None\n",
    "\n",
    "        # Segment embedding\n",
    "        if token_type_ids is not None:\n",
    "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
    "            if mlen > 0:\n",
    "                mem_pad = torch.zeros([mlen, bsz], dtype=torch.long, device=device)\n",
    "                cat_ids = torch.cat([mem_pad, token_type_ids], dim=0)\n",
    "            else:\n",
    "                cat_ids = token_type_ids\n",
    "\n",
    "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
    "            seg_mat = (token_type_ids[:, None] != cat_ids[None, :]).long()\n",
    "            seg_mat = F.one_hot(seg_mat, num_classes=2).to(dtype_float)\n",
    "        else:\n",
    "            seg_mat = None\n",
    "\n",
    "        # Positional encoding\n",
    "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz)\n",
    "        pos_emb = self.dropout(pos_emb)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
    "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
    "        if head_mask is not None:\n",
    "            if head_mask.dim() == 1:\n",
    "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
    "            elif head_mask.dim() == 2:\n",
    "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "            head_mask = head_mask.to(\n",
    "                dtype=next(self.parameters()).dtype\n",
    "            )  # switch to fload if need + fp16 compatibility\n",
    "        else:\n",
    "            head_mask = [None] * self.n_layer\n",
    "\n",
    "        new_mems = ()\n",
    "        if mems is None:\n",
    "            mems = [None] * len(self.layer)\n",
    "\n",
    "        attentions = [] if output_attentions else None\n",
    "        hidden_states = [] if output_hidden_states else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if use_cache:\n",
    "                # cache new mems\n",
    "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
    "            if output_hidden_states:\n",
    "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
    "\n",
    "            outputs = layer_module(\n",
    "                output_h,\n",
    "                output_g,\n",
    "                attn_mask_h=non_tgt_mask,\n",
    "                attn_mask_g=attn_mask,\n",
    "                r=pos_emb,\n",
    "                seg_mat=seg_mat,\n",
    "                mems=mems[i],\n",
    "                target_mapping=target_mapping,\n",
    "                head_mask=head_mask[i],\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            output_h, output_g = outputs[:2]\n",
    "            if output_attentions:\n",
    "                attentions.append(outputs[2])\n",
    "\n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
    "\n",
    "        output = self.dropout(output_g if output_g is not None else output_h)\n",
    "\n",
    "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
    "        output = output.permute(1, 0, 2).contiguous()\n",
    "\n",
    "        if not use_cache:\n",
    "            new_mems = None\n",
    "\n",
    "        if output_hidden_states:\n",
    "            if output_g is not None:\n",
    "                hidden_states = tuple(h.permute(1, 0, 2).contiguous() for hs in hidden_states for h in hs)\n",
    "            else:\n",
    "                hidden_states = tuple(hs.permute(1, 0, 2).contiguous() for hs in hidden_states)\n",
    "\n",
    "        if output_attentions:\n",
    "            if target_mapping is not None:\n",
    "                # when target_mapping is provided, there are 2-tuple of attentions\n",
    "                attentions = tuple(\n",
    "                    tuple(att_stream.permute(2, 3, 0, 1).contiguous() for att_stream in t) for t in attentions\n",
    "                )\n",
    "            else:\n",
    "                attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [output, new_mems, hidden_states, attentions] if v is not None)\n",
    "\n",
    "        return XLNetModelOutput(\n",
    "            last_hidden_state=output, mems=new_mems, hidden_states=hidden_states, attentions=attentions\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
